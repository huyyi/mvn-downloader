import os
import re
import argparse
import random
import requests
import json
import signal
import sys
from collections import defaultdict
from bs4 import BeautifulSoup
from urllib.parse import urljoin, unquote
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
from threading import Lock
import xml.etree.ElementTree as ET
from tqdm import tqdm


class MavenDownloader:
    def __init__(self, base_url="https://repo1.maven.org/maven2/", output_dir="./downloads", max_workers=10, mirrors=None, verbose=False, exclude_patterns=None):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.max_workers = max_workers
        self.verbose = verbose
        self.downloaded_files = set()
        self.lock = Lock()
        self.download_queue = Queue()
        self.pending_files = []  # å¾…ä¸‹è½½æ–‡ä»¶åˆ—è¡¨
        self.interrupted = False  # ä¸­æ–­æ ‡å¿—
        self.new_dependencies = Queue()  # æ–°å‘ç°çš„ä¾èµ– groupId é˜Ÿåˆ—
        self.exclude_patterns = exclude_patterns if exclude_patterns else []  # æ’é™¤æ¨¡å¼åˆ—è¡¨
        
        # çŠ¶æ€æ–‡ä»¶è·¯å¾„
        self.state_dir = self.output_dir / ".mvn-downloader"
        self.state_dir.mkdir(parents=True, exist_ok=True)
        self.downloaded_log = self.state_dir / "downloaded.txt"
        self.pending_log = self.state_dir / "pending.json"
        
        # åŠ è½½å·²ä¸‹è½½æ–‡ä»¶è®°å½•
        self._load_downloaded_files()
        
        # é…ç½®é•œåƒæºåˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨é•œåƒï¼Œå¤±è´¥æ—¶å›é€€åˆ°æºç«™ï¼‰
        if mirrors is None:
            self.mirrors = [
                # é˜¿é‡Œäº‘é•œåƒï¼ˆä¸­å›½å¤§é™†é€Ÿåº¦å¿«ï¼‰
                "https://maven.aliyun.com/repository/public/",
                # ä¸­ç§‘å¤§é•œåƒ
                "https://maven.proxy.ustclug.org/maven2/",
                # åä¸ºäº‘é•œåƒ
                "https://repo.huaweicloud.com/repository/maven/",
                # è…¾è®¯äº‘é•œåƒ
                "https://mirrors.cloud.tencent.com/nexus/repository/maven-public/",
            ]
        else:
            self.mirrors = mirrors if isinstance(mirrors, list) else []
        
        # æ¨¡æ‹Ÿ Maven å®¢æˆ·ç«¯çš„ headers
        self.headers = {
            'User-Agent': 'Apache-Maven/3.9.6 (Java 17.0.9; Linux 5.15.0-91-generic)',
            'Accept': '*/*',
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'en-US,en;q=0.9',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }
    
    def _load_downloaded_files(self):
        """åŠ è½½å·²ä¸‹è½½æ–‡ä»¶è®°å½•"""
        if self.downloaded_log.exists():
            try:
                with open(self.downloaded_log, 'r', encoding='utf-8') as f:
                    for line in f:
                        file_path = line.strip()
                        if file_path:
                            self.downloaded_files.add(file_path)
                if self.downloaded_files:
                    print(f"âœ“ åŠ è½½å·²ä¸‹è½½è®°å½•: {len(self.downloaded_files)} ä¸ªæ–‡ä»¶")
            except Exception as e:
                print(f"âš  åŠ è½½ä¸‹è½½è®°å½•å¤±è´¥: {e}")
    
    def _save_downloaded_file(self, file_path):
        """è®°å½•å·²ä¸‹è½½çš„æ–‡ä»¶"""
        try:
            with open(self.downloaded_log, 'a', encoding='utf-8') as f:
                f.write(f"{file_path}\n")
        except Exception as e:
            print(f"âš  è®°å½•ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
    
    def _save_pending_files(self):
        """ä¿å­˜å¾…ä¸‹è½½æ–‡ä»¶é˜Ÿåˆ—"""
        if self.pending_files:
            try:
                with open(self.pending_log, 'w', encoding='utf-8') as f:
                    json.dump({
                        'files': self.pending_files,
                        'total': len(self.pending_files)
                    }, f, indent=2)
                print(f"\nâœ“ å·²ä¿å­˜å¾…ä¸‹è½½é˜Ÿåˆ—: {len(self.pending_files)} ä¸ªæ–‡ä»¶")
                print(f"  çŠ¶æ€æ–‡ä»¶: {self.pending_log}")
            except Exception as e:
                print(f"\nâš  ä¿å­˜å¾…ä¸‹è½½é˜Ÿåˆ—å¤±è´¥: {e}")
    
    def _load_pending_files(self):
        """åŠ è½½å¾…ä¸‹è½½æ–‡ä»¶é˜Ÿåˆ—"""
        if self.pending_log.exists():
            try:
                with open(self.pending_log, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.pending_files = data.get('files', [])
                if self.pending_files:
                    print(f"âœ“ å‘ç°æœªå®Œæˆçš„ä¸‹è½½ä»»åŠ¡: {len(self.pending_files)} ä¸ªæ–‡ä»¶")
                    response = input("  æ˜¯å¦ç»§ç»­ä¸Šæ¬¡çš„ä¸‹è½½ï¼Ÿ(y/n): ").strip().lower()
                    if response == 'y':
                        return True
                    else:
                        self.pending_files = []
                        self.pending_log.unlink()
                        print("  å·²æ¸…é™¤å¾…ä¸‹è½½é˜Ÿåˆ—")
            except Exception as e:
                print(f"âš  åŠ è½½å¾…ä¸‹è½½é˜Ÿåˆ—å¤±è´¥: {e}")
        return False
    
    def _handle_interrupt(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        print("\n\nâš  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)...")
        self.interrupted = True
        self._save_pending_files()
        print("\næç¤º: ä¸‹æ¬¡è¿è¡Œæ—¶å¯ä»¥ç»§ç»­æœªå®Œæˆçš„ä¸‹è½½")
        sys.exit(0)
        
    def try_request_with_mirrors(self, path, timeout=30, stream=False):
        """éšæœºé€‰æ‹©ä¸€ä¸ªé•œåƒæºä¸‹è½½ï¼Œå¤±è´¥æ—¶ç›´æ¥å›é€€åˆ°æºç«™
        
        Args:
            path: ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ org/springframework/boot/ï¼‰
            timeout: è¶…æ—¶æ—¶é—´
            stream: æ˜¯å¦ä½¿ç”¨æµå¼ä¸‹è½½
            
        Returns:
            (response, source_url) æˆ– (None, None)
        """
        # éšæœºé€‰æ‹©ä¸€ä¸ªé•œåƒæº
        if self.mirrors:
            mirror = random.choice(self.mirrors)
            url = urljoin(mirror, path)
            self._vlog(f"[mirror] {url}")
            try:
                response = requests.get(url, headers=self.headers, timeout=timeout, stream=stream)
                response.raise_for_status()
                return response, mirror
            except Exception as e:
                self._vlog(f"[mirror-fail] {url} -> {e}")
                # é•œåƒå¤±è´¥ï¼Œä¸æ‰“å°é”™è¯¯ï¼Œç›´æ¥å°è¯•æºç«™
                pass
        
        # å›é€€åˆ°æºç«™
        url = urljoin(self.base_url, path)
        self._vlog(f"[origin] {url}")
        try:
            response = requests.get(url, headers=self.headers, timeout=timeout, stream=stream)
            response.raise_for_status()
            return response, self.base_url
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥ï¼ˆæºç«™ï¼‰: {path}, é”™è¯¯: {e}")
            return None, None
    
    def group_id_to_path(self, group_id):
        """å°† groupId è½¬æ¢ä¸ºè·¯å¾„ï¼Œå¦‚ org.springframework.boot -> org/springframework/boot"""
        return group_id.replace('.', '/')
    
    def _should_exclude(self, group_id):
        """æ£€æŸ¥ groupId æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
        if not self.exclude_patterns:
            return False
        
        # å°† groupId åˆ†å‰²æˆéƒ¨åˆ†ï¼Œå¦‚ org.springframework.boot -> ['org', 'springframework', 'boot']
        parts = group_id.split('.')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•éƒ¨åˆ†åŒ¹é…æ’é™¤æ¨¡å¼
        for pattern in self.exclude_patterns:
            # æ”¯æŒå®Œæ•´åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…
            if pattern in parts or any(pattern in part for part in parts):
                return True
        
        return False
    
    def get_artifacts_list(self, group_path):
        """è·å–æŒ‡å®š group è·¯å¾„ä¸‹çš„æ‰€æœ‰ artifact"""
        """è·å–æŒ‡å®š group è·¯å¾„ä¸‹çš„æ‰€æœ‰ artifactï¼Œè¿”å› (artifacts, subgroups)"""
        response, source = self.try_request_with_mirrors(group_path + '/')
        if response is None:
            return [], []
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            artifacts = []
            subgroups = []
            
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and href.endswith('/') and href not in ['../', '../', '/']:
                    item_name = href.rstrip('/')
                    full_path = f"{group_path}/{item_name}"
                    
                    if self._is_artifact_directory(full_path):
                        artifacts.append(full_path)
                    else:
                        subgroups.append(full_path)
            
            return artifacts, subgroups
        except Exception as e:
            print(f"è§£æ artifact åˆ—è¡¨å¤±è´¥: {e}")
            return [], []
    
    def _is_artifact_directory(self, path):
        """åˆ¤æ–­æ˜¯å¦ä¸º artifact ç›®å½•ï¼šä¼˜å…ˆä¾æ® maven-metadataï¼Œå…¶æ¬¡çœ‹æ˜¯å¦å­˜åœ¨ç‰ˆæœ¬ç›®å½•"""
        response, source = self.try_request_with_mirrors(path + '/')
        if response is None:
            return False
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            has_version_dir = False
            for link in soup.find_all('a'):
                href = link.get('href')
                if not href:
                    continue
                # åªè¦å­˜åœ¨ maven-metadata*ï¼ˆæ’é™¤ç­¾åæ–‡ä»¶ï¼‰ï¼Œå³å¯è®¤å®šä¸º artifact ç›®å½•
                if href.startswith('maven-metadata') and not href.endswith('.asc'):
                    return True
                if href.endswith('/') and self._is_version_directory(href.rstrip('/')):
                    has_version_dir = True
            return has_version_dir
        except Exception:
            return False
    
    def _is_version_directory(self, dirname):
        """åˆ¤æ–­ç›®å½•åæ˜¯å¦åƒç‰ˆæœ¬å·"""
        return any(char.isdigit() for char in dirname)

    def _vlog(self, message):
        """verbose æ—¥å¿—è¾“å‡º"""
        if self.verbose:
            print(message)

    def _print_tree(self, group_id, artifacts, versions_map):
        """ä»¥æ ‘å½¢ç»“æ„æ‰“å°å¾…ä¸‹è½½çš„æ–‡ä»¶è®¡åˆ’"""
        print(f"\nâ””â”€ {group_id}")
        for artifact in sorted(artifacts):
            artifact_name = artifact.split('/')[-1]
            print(f"   â”œâ”€ {artifact_name}")
            versions = versions_map.get(artifact, [])
            for idx, (version_path, files) in enumerate(sorted(versions, key=lambda x: x[0])):
                connector = "â””" if idx == len(versions) - 1 else "â”œ"
                version_name = version_path.split('/')[-1]
                print(f"   â”‚  {connector}â”€ {version_name}")
                file_connector_prefix = "   â”‚     " if connector == "â”œ" else "   â”‚     "
                for f in sorted(files):
                    fname = f.split('/')[-1]
                    print(f"{file_connector_prefix}â””â”€ {fname}")
    
    def get_versions_list(self, artifact_path):
        """è·å–æŒ‡å®š artifact çš„æ‰€æœ‰ç‰ˆæœ¬"""
        response, source = self.try_request_with_mirrors(artifact_path + '/')
        if response is None:
            return []
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            versions = []
            
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and href.endswith('/') and href not in ['../', '../', '/']:
                    version = href.rstrip('/')
                    # è·³è¿‡ maven-metadata ç­‰ç‰¹æ®Šç›®å½•
                    if not version.startswith('maven-metadata'):
                        versions.append(f"{artifact_path}/{version}")
            
            return versions
        except Exception as e:
            print(f"è§£æç‰ˆæœ¬åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_files_in_version(self, version_path):
        """è·å–æŒ‡å®šç‰ˆæœ¬ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        response, source = self.try_request_with_mirrors(version_path + '/')
        if response is None:
            return []
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            files = []
            
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and not href.endswith('/') and href not in ['../', '../']:
                    # è·³è¿‡å“ˆå¸Œæ–‡ä»¶ï¼Œä½†ä¿ç•™ maven-metadata.xml
                    if not href.endswith(('.md5', '.sha1', '.sha256', '.sha512', '.asc')):
                        files.append(f"{version_path}/{href}")
            
            return files
        except Exception as e:
            print(f"è§£ææ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_artifact_metadata(self, artifact_path):
        """è·å– artifact çº§åˆ«çš„ maven-metadata.xml æ–‡ä»¶"""
        response, source = self.try_request_with_mirrors(artifact_path + '/')
        if response is None:
            return []
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            metadata_files = []
            
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and href.startswith('maven-metadata'):
                    # æ”¶é›† maven-metadata.xml åŠå…¶æ­¾åæ–‡ä»¶ï¼Œä½†ä¸æ”¶é›†ç­¾åæ–‡ä»¶
                    if not href.endswith(('.asc',)):
                        metadata_files.append(f"{artifact_path}/{href}")
            
            return metadata_files
        except Exception as e:
            print(f"è§£æ artifact å…ƒæ•°æ®å¤±è´¥: {e}")
            return []
    
    def parse_pom_dependencies(self, pom_content):
        """è§£æ POM æ–‡ä»¶ä¸­çš„ä¾èµ–ï¼Œåªæå– groupId"""
        group_ids = set()
        
        try:
            # ç§»é™¤å‘½åç©ºé—´ä»¥ç®€åŒ–è§£æ
            pom_content = re.sub(r'xmlns="[^"]+"', '', pom_content)
            root = ET.fromstring(pom_content)
            
            # æŸ¥æ‰¾æ‰€æœ‰ dependency æ ‡ç­¾ï¼Œåªæå– groupId
            for dependency in root.findall('.//dependency'):
                group_id = dependency.find('groupId')
                
                if group_id is not None and group_id.text:
                    # è¿‡æ»¤æ‰å ä½ç¬¦å˜é‡ï¼ˆå¦‚ ${project.groupId}ï¼‰
                    if not group_id.text.startswith('${'):
                        group_ids.add(group_id.text)
            
            return list(group_ids)
        except Exception as e:
            print(f"è§£æ POM æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def download_file(self, file_path, progress_bar=None):
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
        with self.lock:
            if file_path in self.downloaded_files:
                if progress_bar:
                    progress_bar.update(1)
                return None
            self.downloaded_files.add(file_path)
        
        local_path = self.output_dir / file_path
        
        # åˆ›å»ºç›®å½•
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
        if local_path.exists():
            self._save_downloaded_file(file_path)
            if progress_bar:
                progress_bar.set_postfix_str(f"è·³è¿‡: {file_path[-50:]}")
                progress_bar.update(1)
            # å¦‚æœæ˜¯ POM æ–‡ä»¶ï¼Œä¹Ÿè¦è§£æä¾èµ–
            if file_path.endswith('.pom'):
                with open(local_path, 'r', encoding='utf-8') as f:
                    pom_content = f.read()
                group_ids = self.parse_pom_dependencies(pom_content)
                for group_id in group_ids:
                    self.new_dependencies.put(group_id)
            return None
        
        if progress_bar:
            progress_bar.set_postfix_str(f"ä¸‹è½½: {file_path[-50:]}")
        
        response, source = self.try_request_with_mirrors(file_path, timeout=60, stream=True)
        self._vlog(f"[download] {file_path} from {source}")
        
        if response is None:
            if progress_bar:
                progress_bar.update(1)
            return None
        
        try:
            # ä½¿ç”¨æµå¼ä¸‹è½½ï¼Œåˆ†å—å†™å…¥ï¼ˆå¯¹äºå¤§æ–‡ä»¶æ›´é«˜æ•ˆï¼‰
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # è®°å½•å·²ä¸‹è½½æ–‡ä»¶
            self._save_downloaded_file(file_path)
            
            if progress_bar:
                progress_bar.update(1)
            
            # å¦‚æœæ˜¯ POM æ–‡ä»¶ï¼Œç«‹å³è§£æä¾èµ–å¹¶åŠ å…¥é˜Ÿåˆ—
            if file_path.endswith('.pom'):
                with open(local_path, 'r', encoding='utf-8') as f:
                    pom_content = f.read()
                group_ids = self.parse_pom_dependencies(pom_content)
                # å°†æ–°å‘ç°çš„ä¾èµ–åŠ å…¥é˜Ÿåˆ—ï¼ˆåœ¨çº¿ç¨‹å†…éƒ¨ï¼‰
                for group_id in group_ids:
                    self.new_dependencies.put(group_id)
            
            return {'path': local_path, 'group_ids': []}
            
        except Exception as e:
            if progress_bar:
                progress_bar.write(f"âœ— ä¿å­˜å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                progress_bar.update(1)
            return None
    
    def download_group(self, group_id, include_dependencies=True, max_depth=2, dry_run=False, _current_depth=0, _processed_groups=None):
        """ä¸‹è½½æŒ‡å®š groupId çš„æ‰€æœ‰åŒ…
        
        Args:
            group_id: Maven groupId
            include_dependencies: æ˜¯å¦ä¸‹è½½ä¾èµ–
            max_depth: æœ€å¤§é€’å½’æ·±åº¦ï¼Œé¿å…æ— é™é€’å½’ï¼ˆé»˜è®¤2å±‚ï¼‰
            _current_depth: å½“å‰é€’å½’æ·±åº¦ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
            _processed_groups: å·²å¤„ç†çš„ groupId é›†åˆï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        """
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼ˆä»…åœ¨é¡¶å±‚ï¼‰
        if _current_depth == 0:
            signal.signal(signal.SIGINT, self._handle_interrupt)
            
            # å°è¯•æ¢å¤ä¸Šæ¬¡æœªå®Œæˆçš„ä¸‹è½½
            if self._load_pending_files():
                return self._resume_download()
        
        # åˆå§‹åŒ–å·²å¤„ç†é›†åˆ
        if _processed_groups is None:
            _processed_groups = set()
        
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡æˆ–è¶…è¿‡æœ€å¤§æ·±åº¦
        if group_id in _processed_groups:
            return
        
        if _current_depth >= max_depth:
            return
        
        _processed_groups.add(group_id)
        group_path = self.group_id_to_path(group_id)
        
        indent = "  " * _current_depth
        print(f"\n{indent}{'=' * 60}")
        print(f"{indent}[æ·±åº¦ {_current_depth}] ğŸ“¦ {group_id}")
        if _current_depth == 0:
            print(f"{indent}ğŸ“ è¾“å‡º: {self.output_dir}")
            print(f"{indent}ğŸ”§ çº¿ç¨‹: {self.max_workers}")
            print(f"{indent}ğŸŒ é•œåƒ: {len(self.mirrors)} ä¸ª + æºç«™")
            print(f"{indent}ğŸ“Š å·²ä¸‹è½½: {len(self.downloaded_files)} ä¸ªæ–‡ä»¶")
        print(f"{indent}{'=' * 60}")
        
        # 1. è·å–æ‰€æœ‰ artifacts
        print(f"{indent}ğŸ” æ‰«æ artifacts...")
        artifacts, subgroups = self.get_artifacts_list(group_path)
        
        # è¿‡æ»¤æ’é™¤çš„ artifacts å’Œ subgroups
        excluded_artifacts = []
        excluded_subgroups = []
        
        if self.exclude_patterns:
            # è¿‡æ»¤ artifacts
            filtered_artifacts = []
            for artifact in artifacts:
                artifact_id = artifact.replace('/', '.')
                if self._should_exclude(artifact_id):
                    excluded_artifacts.append(artifact)
                else:
                    filtered_artifacts.append(artifact)
            artifacts = filtered_artifacts
            
            # è¿‡æ»¤ subgroups
            filtered_subgroups = []
            for subgroup in subgroups:
                subgroup_id = subgroup.replace('/', '.')
                if self._should_exclude(subgroup_id):
                    excluded_subgroups.append(subgroup)
                else:
                    filtered_subgroups.append(subgroup)
            subgroups = filtered_subgroups
            
            # æ‰“å°æ’é™¤ä¿¡æ¯
            if excluded_artifacts or excluded_subgroups:
                total_excluded = len(excluded_artifacts) + len(excluded_subgroups)
                print(f"{indent}âŠ˜ æ’é™¤ {total_excluded} ä¸ªé¡¹ç›® (artifacts: {len(excluded_artifacts)}, subgroups: {len(excluded_subgroups)})")
        
        if not artifacts and subgroups:
            print(f"{indent}ğŸ“‚ æ‰¾åˆ° {len(subgroups)} ä¸ªå­groupï¼Œç»§ç»­æ¢ç´¢...")
            for subgroup_path in subgroups:
                subgroup_id = subgroup_path.replace('/', '.')
                self.download_group(
                    group_id=subgroup_id,
                    include_dependencies=include_dependencies,
                    max_depth=max_depth,
                    dry_run=dry_run,
                    _current_depth=_current_depth,
                    _processed_groups=_processed_groups
                )
            return
        
        if not artifacts:
            print(f"{indent}âš  æœªæ‰¾åˆ°ä»»ä½• artifact æˆ–å­group")
            return
        print(f"{indent}âœ“ æ‰¾åˆ° {len(artifacts)} ä¸ª artifact")
        
        if subgroups:
            print(f"{indent}ğŸ“‚ åŒæ—¶æ‰¾åˆ° {len(subgroups)} ä¸ªå­group")
            for subgroup_path in subgroups:
                subgroup_id = subgroup_path.replace('/', '.')
                self.download_group(
                    group_id=subgroup_id,
                    include_dependencies=include_dependencies,
                    max_depth=max_depth,
                    dry_run=dry_run,
                    _current_depth=_current_depth,
                    _processed_groups=_processed_groups
                )
        
        # 2. è·å–æ‰€æœ‰ç‰ˆæœ¬
        print(f"{indent}ğŸ” æ‰«æç‰ˆæœ¬...", end='', flush=True)
        all_versions = []
        versions_map = defaultdict(list)  # artifact -> list[(version_path, files)]
        for artifact in artifacts:
            versions = self.get_versions_list(artifact)
            all_versions.extend(versions)
            for v in versions:
                versions_map[artifact].append((v, []))
        print(f"\r{indent}âœ“ æ‰¾åˆ° {len(all_versions)} ä¸ªç‰ˆæœ¬" + " " * 20)
        
        # 3. è·å–æ‰€æœ‰æ–‡ä»¶
        print(f"{indent}ğŸ” æ‰«ææ–‡ä»¶...", end='', flush=True)
        all_files = []
        for version in all_versions:
            files = self.get_files_in_version(version)
            all_files.extend(files)
            # è®°å½•ç‰ˆæœ¬æ–‡ä»¶åˆ° map
            for artifact in artifacts:
                if version.startswith(artifact + '/'):
                    versions_map[artifact] = [
                        (v_path, files if v_path == version else v_files)
                        for (v_path, v_files) in versions_map[artifact]
                    ]
                    break
        
        # è·å– artifact çº§åˆ«çš„ maven-metadata.xml
        for artifact in artifacts:
            metadata_files = self.get_artifact_metadata(artifact)
            all_files.extend(metadata_files)
            if metadata_files:
                versions_map[artifact].append((f"{artifact}/maven-metadata", metadata_files))
        
        print(f"\r{indent}âœ“ æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶" + " " * 20)
        
        if not all_files:
            print(f"{indent}âš  æ²¡æœ‰æ–‡ä»¶éœ€è¦ä¸‹è½½")
            return

        # dry-run æ¨¡å¼ï¼šä»…æ‰“å°è®¡åˆ’ï¼Œä¸ä¸‹è½½
        if dry_run:
            print(f"{indent}ğŸ“„ Dry-run æ¨¡å¼ï¼Œä»…å±•ç¤ºå¾…ä¸‹è½½æ–‡ä»¶ï¼š")
            self._print_tree(group_id, artifacts, versions_map)
            return
        
        # è¿‡æ»¤å·²ä¸‹è½½çš„æ–‡ä»¶
        files_to_download = [f for f in all_files if f not in self.downloaded_files]
        if not files_to_download:
            print(f"{indent}âœ“ æ‰€æœ‰æ–‡ä»¶å·²ä¸‹è½½")
        else:
            print(f"{indent}ğŸ“¥ éœ€è¦ä¸‹è½½ {len(files_to_download)} ä¸ªæ–‡ä»¶ (è·³è¿‡ {len(all_files) - len(files_to_download)} ä¸ªå·²ä¸‹è½½)")
        
        # ä¿å­˜å¾…ä¸‹è½½åˆ—è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        self.pending_files = files_to_download
        
        # 4. å¤šçº¿ç¨‹ä¸‹è½½ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        downloaded_poms = []
        if files_to_download:
            print(f"{indent}â¬‡ï¸  å¼€å§‹ä¸‹è½½...")
            with tqdm(total=len(files_to_download), desc=f"{indent}ä¸‹è½½è¿›åº¦", 
                     unit="æ–‡ä»¶", ncols=100, leave=True) as pbar:
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_file = {executor.submit(self.download_file, file_path, pbar): file_path 
                                    for file_path in files_to_download}
                    
                    for future in as_completed(future_to_file):
                        if self.interrupted:
                            executor.shutdown(wait=False)
                            return
                        result = future.result()
                        if result and result.get('group_ids'):
                            downloaded_poms.append(result)
        
        # æ¸…é™¤å¾…ä¸‹è½½åˆ—è¡¨
        self.pending_files = []
        if self.pending_log.exists():
            self.pending_log.unlink()
        
        print(f"{indent}âœ“ æœ¬æ¬¡ä¸‹è½½å®Œæˆ")
        
        # 5. é€’å½’å¤„ç†ä¾èµ–ï¼ˆå¦‚æœéœ€è¦ä¸”æœªè¾¾åˆ°æœ€å¤§æ·±åº¦ï¼‰
        if include_dependencies and _current_depth < max_depth - 1:
            print(f"\n{indent}ğŸ”— å¤„ç†ä¾èµ–...")
            dependency_groups = set()
            
            # æ”¶é›†æ‰€æœ‰æ–°å‘ç°çš„ä¾èµ– groupIdï¼ˆä»çº¿ç¨‹å†…éƒ¨é˜Ÿåˆ—ä¸­ï¼‰
            while not self.new_dependencies.empty():
                dep_group_id = self.new_dependencies.get()
                if dep_group_id not in _processed_groups:
                    dependency_groups.add(dep_group_id)
            
            if dependency_groups:
                print(f"{indent}âœ“ å‘ç° {len(dependency_groups)} ä¸ªä¾èµ– group:")
                for dep_group in sorted(dependency_groups):
                    print(f"{indent}  â€¢ {dep_group}")
                
                # é€’å½’ä¸‹è½½æ¯ä¸ªä¾èµ– group
                for dep_group_id in sorted(dependency_groups):
                    self.download_group(
                        group_id=dep_group_id,
                        include_dependencies=include_dependencies,
                        max_depth=max_depth,
                        dry_run=dry_run,
                        _current_depth=_current_depth + 1,
                        _processed_groups=_processed_groups
                    )
        
        # åªåœ¨é¡¶å±‚æ‰“å°æ€»ç»“
        if _current_depth == 0:
            print("\n" + "=" * 60)
            print("âœ… å…¨éƒ¨ä¸‹è½½å®Œæˆï¼")
            print(f"  ğŸ“Š å¤„ç†äº† {len(_processed_groups)} ä¸ª group")
            print(f"  ğŸ“¥ å…±ä¸‹è½½ {len(self.downloaded_files)} ä¸ªæ–‡ä»¶")
            print(f"  ğŸ“ ä¿å­˜ä½ç½®: {self.output_dir.absolute()}")
            print("=" * 60)
    
    def _resume_download(self):
        """æ¢å¤ä¸­æ–­çš„ä¸‹è½½ä»»åŠ¡"""
        if not self.pending_files:
            return
        
        print(f"\n{'=' * 60}")
        print("ğŸ”„ æ¢å¤ä¸‹è½½ä»»åŠ¡")
        print(f"  ğŸ“¥ å¾…ä¸‹è½½: {len(self.pending_files)} ä¸ªæ–‡ä»¶")
        print(f"{'=' * 60}\n")
        
        downloaded_poms = []
        with tqdm(total=len(self.pending_files), desc="æ¢å¤ä¸‹è½½", 
                 unit="æ–‡ä»¶", ncols=100) as pbar:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_file = {executor.submit(self.download_file, file_path, pbar): file_path 
                                for file_path in self.pending_files}
                
                for future in as_completed(future_to_file):
                    if self.interrupted:
                        executor.shutdown(wait=False)
                        return
                    result = future.result()
                    if result and result.get('group_ids'):
                        downloaded_poms.append(result)
        
        # æ¸…é™¤å¾…ä¸‹è½½åˆ—è¡¨
        self.pending_files = []
        if self.pending_log.exists():
            self.pending_log.unlink()
        
        print("\n" + "=" * 60)
        print("âœ… æ¢å¤ä¸‹è½½å®Œæˆï¼")
        print(f"  ğŸ“¥ å…±ä¸‹è½½ {len(self.downloaded_files)} ä¸ªæ–‡ä»¶")
        print(f"  ğŸ“ ä¿å­˜ä½ç½®: {self.output_dir.absolute()}")
        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='ä» Maven ä»“åº“ä¸‹è½½æŒ‡å®š groupId çš„æ‰€æœ‰åŒ…')
    parser.add_argument('group_id', help='Maven groupIdï¼Œå¦‚: org.springframework.boot')
    parser.add_argument('-o', '--output', default='./downloads', help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ./downloadsï¼‰')
    parser.add_argument('-w', '--workers', type=int, default=10, help='çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('-d', '--depth', type=int, default=2, help='ä¾èµ–é€’å½’æ·±åº¦ï¼ˆé»˜è®¤: 2ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ‰“å°å¾…ä¸‹è½½åˆ—è¡¨ï¼ˆtree æ ¼å¼ï¼‰ï¼Œä¸å®é™…ä¸‹è½½')
    parser.add_argument('-m', '--mirrors', nargs='*', help='è‡ªå®šä¹‰é•œåƒæºåˆ—è¡¨ï¼ˆå¤šä¸ªURLç”¨ç©ºæ ¼åˆ†éš”ï¼‰')
    parser.add_argument('--no-mirrors', action='store_true', help='ä¸ä½¿ç”¨é•œåƒæºï¼Œç›´æ¥ä»æºç«™ä¸‹è½½')
    parser.add_argument('--no-deps', action='store_true', help='ä¸è§£æä¾èµ–')
    parser.add_argument('-e', '--exclude', nargs='*', help='æ’é™¤çš„ subgroup æ¨¡å¼åˆ—è¡¨ï¼ˆå¦‚: boot dataï¼‰')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ˆé•œåƒé€‰æ‹©ã€ä¸‹è½½æ¥æºç­‰ï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç†é•œåƒæºé…ç½®
    mirrors = None
    if args.no_mirrors:
        mirrors = []  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨é•œåƒ
    elif args.mirrors:
        mirrors = args.mirrors
    # å¦åˆ™ä½¿ç”¨é»˜è®¤é•œåƒåˆ—è¡¨
    
    downloader = MavenDownloader(
        output_dir=args.output,
        max_workers=args.workers,
        mirrors=mirrors,
        verbose=args.verbose,
        exclude_patterns=args.exclude
    )
    
    downloader.download_group(
        group_id=args.group_id,
        include_dependencies=not args.no_deps,
        max_depth=args.depth,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()
